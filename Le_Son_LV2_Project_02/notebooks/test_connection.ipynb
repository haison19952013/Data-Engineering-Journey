{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56d9b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")  # Add parent directory to sys.path for module imports\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StringType, StructType, StructField, LongType, ArrayType, MapType\n",
    "from config import load_config\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, BooleanType, ArrayType, MapType\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from utils import extract_browser, extract_os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc3ebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_conf = load_config(filename='../config.ini',section = 'remote_kafka')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07beb486",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SimpleApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fcce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df):\n",
    "    \"\"\"Transform raw data by adding computed columns.\"\"\"\n",
    "    \n",
    "    # Convert timestamp to datetime (assuming seconds since epoch)\n",
    "    df['timestamp_dt'] = pd.to_datetime(df['time_stamp'], unit='s')\n",
    "    \n",
    "    # Create date and time columns\n",
    "    df['full_date'] = df['timestamp_dt'].dt.strftime('%Y-%m-%d')\n",
    "    df['full_time'] = df['timestamp_dt'].dt.strftime('%H:%M:%S')\n",
    "    \n",
    "    # Create hash keys\n",
    "    df['sales_key'] = df.apply(lambda row: hashlib.sha256(f\"{row['id']}{row['product_id']}\".encode()).hexdigest(), axis=1)\n",
    "    df['ip_key'] = df['ip'].apply(lambda x: hashlib.sha256(x.encode()).hexdigest() if pd.notna(x) else None)\n",
    "    df['user_agent_key'] = df['user_agent'].apply(lambda x: hashlib.sha256(x.encode()).hexdigest() if pd.notna(x) else None)\n",
    "    df['product_key'] = df['product_id']\n",
    "    \n",
    "    # Extract browser and OS from user_agent\n",
    "    df['browser'] = df['user_agent'].apply(extract_browser)\n",
    "    df['os'] = df['user_agent'].apply(extract_os)\n",
    "    \n",
    "    # Convert option array to string\n",
    "    df['option'] = df['option'].apply(lambda x: str(x) if x is not None else None)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eee51bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.streaming import StreamingQuery\n",
    "\n",
    "# Alternative approach using foreachBatch for better compatibility\n",
    "def process_batch(df, epoch_id):\n",
    "    \"\"\"Process each micro-batch of data.\"\"\"\n",
    "    if df.count() > 0:\n",
    "        # Convert to pandas DataFrame\n",
    "        pandas_df = df.toPandas()\n",
    "        \n",
    "        # Apply transform function\n",
    "        transformed_df = transform(pandas_df)\n",
    "        \n",
    "        # Print transformed data (for testing)\n",
    "        print(f\"Batch {epoch_id} - Transformed Data Sample:\")\n",
    "        print(transformed_df.head(3))\n",
    "        print(f\"Total records in batch: {len(transformed_df)}\")\n",
    "        print(\"---\")\n",
    "\n",
    "# Read from Kafka\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .options(**kafka_conf) \\\n",
    "    .load()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"_id\", StringType()),\n",
    "    StructField(\"time_stamp\", LongType()),\n",
    "    StructField(\"ip\", StringType()),\n",
    "    StructField(\"user_agent\", StringType()),\n",
    "    StructField(\"resolution\", StringType()),\n",
    "    StructField(\"user_id_db\", StringType()),\n",
    "    StructField(\"device_id\", StringType()),\n",
    "    StructField(\"api_version\", StringType()),\n",
    "    StructField(\"store_id\", StringType()),\n",
    "    StructField(\"local_time\", StringType()),\n",
    "    StructField(\"show_recommendation\", StringType()),\n",
    "    StructField(\"current_url\", StringType()),\n",
    "    StructField(\"referrer_url\", StringType()),\n",
    "    StructField(\"email_address\", StringType()),\n",
    "    StructField(\"recommendation\", StringType()),\n",
    "    StructField(\"utm_source\", StringType()),\n",
    "    StructField(\"utm_medium\", StringType()),\n",
    "    StructField(\"collection\", StringType()),\n",
    "    StructField(\"product_id\", StringType()),\n",
    "    StructField(\"option\", ArrayType(MapType(StringType(), StringType()))),\n",
    "    StructField(\"id\", StringType())\n",
    "])\n",
    "\n",
    "# Parse JSON data\n",
    "parsed_df = df.selectExpr(\"CAST(value AS STRING) as json_value\") \\\n",
    "    .select(from_json(col(\"json_value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Apply transform function via foreachBatch (more compatible)\n",
    "query = parsed_df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/spark_checkpoints/test_notebook\") \\\n",
    "    .trigger(processingTime=\"2 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Streaming pipeline with transform started. Press Ctrl+C to stop.\")\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
